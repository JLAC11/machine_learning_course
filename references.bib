
@article{beretta_nearest_2016,
  title = {Nearest Neighbor Imputation Algorithms: A Critical Evaluation},
  shorttitle = {Nearest Neighbor Imputation Algorithms},
  author = {Beretta, Lorenzo and Santaniello, Alessandro},
  date = {2016-07-25},
  journaltitle = {BMC Medical Informatics and Decision Making},
  shortjournal = {BMC Medical Informatics and Decision Making},
  volume = {16},
  number = {3},
  pages = {74},
  issn = {1472-6947},
  doi = {10.1186/s12911-016-0318-z},
  url = {https://doi.org/10.1186/s12911-016-0318-z},
  urldate = {2022-02-26},
  abstract = {Nearest neighbor (NN) imputation algorithms are efficient methods to fill in missing data where each missing value on some records is replaced by a value obtained from related cases in the whole set of records. Besides the capability to substitute the missing data with plausible values that are as close as possible to the true value, imputation algorithms should preserve the original data structure and avoid to distort the distribution of the imputed variable. Despite the efficiency of NN algorithms little is known about the effect of these methods on data structure.},
  keywords = {Imputation Algorithm,Imputation Method,Minkowski Norm,Near Neighbour,Near Neighbour Algorithm},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/793XGGNS/Beretta and Santaniello - 2016 - Nearest neighbor imputation algorithms a critical.pdf;/Users/joseluisaguilarcharfen/Zotero/storage/5A82K4M5/s12911-016-0318-z.html}
}

@online{blackburn_introduction_2021,
  title = {Introduction to {{Reinforcement Learning}} : {{Markov-Decision Process}}},
  shorttitle = {Introduction to {{Reinforcement Learning}}},
  author = {{blackburn}},
  date = {2021-07-25T08:02:49},
  url = {https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da},
  urldate = {2022-03-28},
  abstract = {In a typical Reinforcement Learning (RL) problem, there is a learner and a decision maker called agent and the surrounding with which it…},
  langid = {english},
  organization = {{Medium}}
}

@online{blackburn_introduction_2021-1,
  title = {Introduction to {{Reinforcement Learning}} : {{Markov-Decision Process}}},
  shorttitle = {Introduction to {{Reinforcement Learning}}},
  author = {{blackburn}},
  date = {2021-07-25T08:02:49},
  url = {https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da},
  urldate = {2022-03-28},
  abstract = {In a typical Reinforcement Learning (RL) problem, there is a learner and a decision maker called agent and the surrounding with which it…},
  langid = {english},
  organization = {{Medium}}
}

@article{bonhommeau_eating_2013,
  title = {Eating up the World’s Food Web and the Human Trophic Level},
  author = {Bonhommeau, Sylvain and Dubroca, Laurent and Pape, Olivier Le and Barde, Julien and Kaplan, David M. and Chassot, Emmanuel and Nieblas, Anne-Elise},
  date = {2013-12-17},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {110},
  number = {51},
  eprint = {24297882},
  eprinttype = {pmid},
  pages = {20617--20620},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1305827110},
  url = {https://www.pnas.org/content/110/51/20617},
  urldate = {2022-02-28},
  abstract = {Trophic levels are critical for synthesizing species’ diets, depicting energy pathways, understanding food web dynamics and ecosystem functioning, and monitoring ecosystem health. Specifically, trophic levels describe the position of species in a food web, from primary producers to apex predators (range, 1–5). Small differences in trophic level can reflect large differences in diet. Although trophic levels are among the most basic information collected for animals in ecosystems, a human trophic level (HTL) has never been defined. Here, we find a global HTL of 2.21, i.e., the trophic level of anchoveta. This value has increased with time, consistent with the global trend toward diets higher in meat. National HTLs ranging between 2.04 and 2.57 reflect a broad diversity of diet, although cluster analysis of countries with similar dietary trends reveals only five major groups. We find significant links between socio-economic and environmental indicators and global dietary trends. We demonstrate that the HTL is a synthetic index to monitor human diets and provides a baseline to compare diets between countries.},
  langid = {english},
  keywords = {human ecology,nutrition transition,trophic ecology},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/LHJ4BSI9/Bonhommeau et al. - 2013 - Eating up the world’s food web and the human troph.pdf;/Users/joseluisaguilarcharfen/Zotero/storage/KGWBUX2N/20617.html}
}

@book{d_reinforcement_2020,
  title = {Reinforcement {{Learning}}: {{Industrial Applications}} of {{Intelligent Agents}}},
  shorttitle = {Reinforcement {{Learning}}},
  author = {D, Phil Winder Ph},
  date = {2020-12-01},
  edition = {1st edition},
  publisher = {{O'Reilly Media}},
  location = {{S.l.}},
  isbn = {978-1-09-811483-1},
  langid = {english},
  pagetotal = {408}
}

@inproceedings{jaakkola_convergence_1993,
  title = {Convergence of {{Stochastic Iterative Dynamic Programming Algorithms}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jaakkola, Tommi and Jordan, Michael and Singh, Satinder},
  date = {1993},
  volume = {6},
  publisher = {{Morgan-Kaufmann}},
  url = {https://proceedings.neurips.cc/paper/1993/hash/5807a685d1a9ab3b599035bc566ce2b9-Abstract.html},
  urldate = {2022-03-28},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/NQUJXMBM/Jaakkola et al. - 1993 - Convergence of Stochastic Iterative Dynamic Progra.pdf}
}

@article{melo_convergence_nodate,
  title = {Convergence of {{Q-learning}}: A Simple Proof},
  author = {Melo, Francisco S},
  pages = {4},
  langid = {english},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/ATE9JF59/Melo - Convergence of Q-learning a simple proof.pdf}
}

@online{noauthor_calculating_nodate,
  title = {Calculating Efficiency of Biomass Transfers - {{Trophic}} Levels in an Ecosystem - {{AQA}} - {{GCSE Biology}} ({{Single Science}}) {{Revision}} - {{AQA}}},
  url = {https://www.bbc.co.uk/bitesize/guides/zs7gw6f/revision/4},
  urldate = {2022-02-26},
  abstract = {Learn about how feeding relationships are shown in food chains for GCSE Biology, AQA.},
  langid = {british},
  organization = {{BBC Bitesize}},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/3IP7R4AZ/4.html}
}

@online{noauthor_home_nodate,
  title = {Home - {{Global}} Yield Gap Atlas},
  url = {https://www.yieldgap.org/web/guest/home},
  urldate = {2022-02-27}
}

@online{noauthor_income_nodate,
  title = {Income {{Distribution Database}} : By Country - {{POVERTY}}},
  url = {https://stats.oecd.org/index.aspx?queryid=66598},
  urldate = {2022-02-28}
}

@online{noauthor_income_nodate-1,
  title = {Income {{Distribution Database}} : By Country - {{POVERTY}}},
  url = {https://stats.oecd.org/index.aspx?queryid=66598},
  urldate = {2022-02-28}
}

@online{noauthor_income_nodate-2,
  title = {Income {{Distribution Database}} : By Country - {{POVERTY}}},
  url = {https://stats.oecd.org/index.aspx?queryid=66598},
  urldate = {2022-02-28}
}

@online{noauthor_reinforcement_nodate,
  title = {Reinforcement {{Learning}} : {{Markov-Decision Process}} ({{Part}} 1) | by Blackburn | {{Towards Data Science}}},
  url = {https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da},
  urldate = {2022-03-28},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/KB6GM5YG/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da.html}
}

@online{noauthor_reinforcement_nodate-1,
  title = {Reinforcement {{Learning}} [{{Book}}]},
  url = {https://www.oreilly.com/library/view/reinforcement-learning/9781492072386/},
  urldate = {2022-03-28},
  abstract = {Reinforcement learning (RL) will deliver one of the biggest breakthroughs in AI over the next decade, enabling algorithms to learn from their environment to achieve arbitrary goals. This exciting development … - Selection from Reinforcement Learning [Book]},
  isbn = {9781098114831},
  langid = {english},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/ZL795Q5S/9781492072386.html}
}

@article{roweis_nonlinear_2000,
  title = {Nonlinear {{Dimensionality Reduction}} by {{Locally Linear Embedding}}},
  author = {Roweis, Sam T. and Saul, Lawrence K.},
  date = {2000-12-22},
  journaltitle = {Science},
  volume = {290},
  number = {5500},
  pages = {2323--2326},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.290.5500.2323},
  url = {https://www.science.org/doi/10.1126/science.290.5500.2323},
  urldate = {2022-05-04}
}

@video{steve_brunton_model_2022,
  title = {Model {{Based Reinforcement Learning}}: {{Policy Iteration}}, {{Value Iteration}}, and {{Dynamic Programming}}},
  shorttitle = {Model {{Based Reinforcement Learning}}},
  editor = {{Steve Brunton}},
  date = {2022-01-07},
  url = {https://www.youtube.com/watch?v=sJIFUTITfBc},
  urldate = {2022-03-28},
  abstract = {Here we introduce dynamic programming, which is a cornerstone of model-based reinforcement learning.  We demonstrate dynamic programming for policy iteration and value iteration, leading to the quality function and Q-learning. This is a lecture in a series on reinforcement learning, following the new Chapter 11 from the 2nd edition of our book "Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control"  by Brunton and Kutz Book Website: http://databookuw.com  Book PDF: http://databookuw.com/databook.pdf Amazon: https://www.amazon.com/Data-Driven-Sc... Brunton Website: eigensteve.com},
  editortype = {director}
}

@video{steve_brunton_reinforcement_2022,
  title = {Reinforcement {{Learning Series}}: {{Overview}} of {{Methods}}},
  shorttitle = {Reinforcement {{Learning Series}}},
  editor = {{Steve Brunton}},
  date = {2022-01-03},
  url = {https://www.youtube.com/watch?v=i7q8bISGwMQ},
  urldate = {2022-03-28},
  abstract = {This video introduces the variety of methods for model-based and model-free reinforcement learning, including: dynamic programming, value and policy iteration, Q-learning, deep RL, TD-learning, SARSA, policy gradient optimization, among others.   This is the overview in a series on reinforcement learning, following the new Chapter 11 from the 2nd edition of our book "Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control"  by Brunton and Kutz Book Website: http://databookuw.com  Book PDF: http://databookuw.com/databook.pdf RL Chapter: https://faculty.washington.edu/sbrunt... Amazon: https://www.amazon.com/Data-Driven-Sc... Brunton Website: eigensteve.com},
  editortype = {director}
}

@video{steve_brunton_reinforcement_2022-1,
  title = {Reinforcement {{Learning Series}}: {{Overview}} of {{Methods}}},
  shorttitle = {Reinforcement {{Learning Series}}},
  editor = {{Steve Brunton}},
  date = {2022-01-03},
  url = {https://www.youtube.com/watch?v=i7q8bISGwMQ},
  urldate = {2022-03-28},
  abstract = {This video introduces the variety of methods for model-based and model-free reinforcement learning, including: dynamic programming, value and policy iteration, Q-learning, deep RL, TD-learning, SARSA, policy gradient optimization, among others.   This is the overview in a series on reinforcement learning, following the new Chapter 11 from the 2nd edition of our book "Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control"  by Brunton and Kutz Book Website: http://databookuw.com  Book PDF: http://databookuw.com/databook.pdf RL Chapter: https://faculty.washington.edu/sbrunt... Amazon: https://www.amazon.com/Data-Driven-Sc... Brunton Website: eigensteve.com},
  editortype = {director}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  shorttitle = {Reinforcement {{Learning}}},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018-11-13},
  series = {Adaptive {{Computation}} and {{Machine Learning}} Series},
  edition = {2},
  publisher = {{A Bradford Book}},
  location = {{Cambridge, MA, USA}},
  abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.},
  editorb = {Bach, Francis},
  editorbtype = {redactor},
  isbn = {978-0-262-03924-6},
  langid = {english},
  pagetotal = {552},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/G7MTUZ9L/reinforcement-learning-an-introduction-2.html}
}

@article{sutton_reinforcement_nodate,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  author = {Sutton, Richard S and Barto, Andrew G},
  pages = {352},
  langid = {english},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/JWHKJF4F/Sutton and Barto - Reinforcement Learning An Introduction.pdf}
}

@online{united_nations_development_programme_sustainable_nodate,
  title = {Sustainable {{Development Report}} 2021},
  author = {{United Nations Development Programme}},
  url = {https://dashboards.sdgindex.org/},
  urldate = {2022-02-26},
  abstract = {The Sustainable Development Report 2021 tracks the performance of all 193 UN Member States on the 17 Sustainable Development Goals.},
  langid = {english},
  organization = {{Sustainable Development Report 2021: Downloads}},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/VJMKBZLS/downloads.html}
}

@online{united_nations_development_programme_sustainable_nodate-1,
  title = {Sustainable {{Development Goals}} | {{United Nations Development Programme}}},
  author = {{United Nations Development Programme}},
  url = {https://www.undp.org/sustainable-development-goals?utm_source=EN&utm_medium=GSR&utm_content=US_UNDP_PaidSearch_Brand_English&utm_campaign=CENTRAL&c_src=CENTRAL&c_src2=GSR&gclid=EAIaIQobChMIxePX7YGe9gIVcyCtBh3XCgZREAAYAiAAEgJQV_D_BwE},
  urldate = {2022-02-26},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/7TXLNGHG/sustainable-development-goals.html}
}

@online{valkov_solving_2019,
  title = {Solving an {{MDP}} with {{Q-Learning}} from Scratch — {{Deep Reinforcement Learning}} for {{Hackers}} ({{Part}} 1)},
  author = {Valkov, Venelin},
  date = {2019-04-04T07:11:47},
  url = {https://medium.com/@curiousily/solving-an-mdp-with-q-learning-from-scratch-deep-reinforcement-learning-for-hackers-part-1-45d1d360c120},
  urldate = {2022-03-28},
  abstract = {It is time to learn about value functions, the Bellman equation, and Q-learning. You will use all that knowledge to build an MDP and train…},
  langid = {english},
  organization = {{Medium}}
}

@article{ward_hierarchical_1963,
  title = {Hierarchical {{Grouping}} to {{Optimize}} an {{Objective Function}}},
  author = {Ward, Joe H.},
  date = {1963},
  journaltitle = {Journal of the American Statistical Association},
  volume = {58},
  number = {301},
  eprint = {2282967},
  eprinttype = {jstor},
  pages = {236--244},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2282967},
  abstract = {A procedure for forming hierarchical groups of mutually exclusive subsets, each of which has members that are maximally similar with respect to specified characteristics, is suggested for use in large-scale ({$<$}latex{$>\$$}n {$>$} 100\${$<$}/latex{$>$}) studies when a precise optimal solution for a specified number of groups is not practical. Given n sets, this procedure permits their reduction to n - 1 mutually exclusive sets by considering the union of all possible n(n - 1)/2 pairs and selecting a union having a maximal value for the functional relation, or objective function, that reflects the criterion chosen by the investigator. By repeating this process until only one group remains, the complete hierarchical structure and a quantitative estimate of the loss associated with each stage in the grouping can be obtained. A general flowchart helpful in computer programming and a numerical example are included.}
}

@article{watkins_q-learning_1992,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {279--292},
  issn = {1573-0565},
  doi = {10.1007/BF00992698},
  url = {https://doi.org/10.1007/BF00992698},
  urldate = {2022-03-26},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  langid = {english},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/9VTWXHZF/Watkins and Dayan - 1992 - Q-learning.pdf}
}

@article{watkins_q-learning_1992-1,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {279--292},
  issn = {1573-0565},
  doi = {10.1007/BF00992698},
  url = {https://doi.org/10.1007/BF00992698},
  urldate = {2022-03-28},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  langid = {english},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/CQ7YP5MN/Watkins and Dayan - 1992 - Q-learning.pdf}
}

@online{weng_long_2018,
  title = {A ({{Long}}) {{Peek}} into {{Reinforcement Learning}}},
  author = {Weng, Lilian},
  date = {2018-02-19T00:00:00+00:00},
  url = {https://lilianweng.github.io/posts/2018-02-19-rl-overview/},
  urldate = {2022-03-28},
  abstract = {[Updated on 2020-09-03: Updated the algorithm of SARSA and Q-learning so that the difference is more pronounced. [Updated on 2021-09-19: Thanks to 爱吃猫的鱼, we have this post in Chinese]. A couple of exciting news in Artificial Intelligence (AI) has just happened in recent years. AlphaGo defeated the best professional human player in the game of Go. Very soon the extended algorithm AlphaGo Zero beat AlphaGo by 100-0 without supervised learning on human knowledge.},
  langid = {english},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/STFVZUVU/2018-02-19-rl-overview.html}
}

@online{weng_policy_2018,
  title = {Policy {{Gradient Algorithms}}},
  author = {Weng, Lilian},
  date = {2018-04-08T00:00:00+00:00},
  url = {https://lilianweng.github.io/posts/2018-04-08-policy-gradient/},
  urldate = {2022-03-28},
  abstract = {[Updated on 2018-06-30: add two new policy gradient methods, SAC and D4PG.]  [Updated on 2018-09-30: add a new policy gradient method, TD3.]  [Updated on 2019-02-09: add SAC with automatically adjusted temperature].  [Updated on 2019-06-26: Thanks to Chanseok, we have a version of this post in Korean].  [Updated on 2019-09-12: add a new policy gradient method SVPG.]  [Updated on 2019-12-22: add a new policy gradient method IMPALA.},
  langid = {english},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/LYH6TTSK/2018-04-08-policy-gradient.html}
}

@article{yeo_new_2000,
  title = {A New Family of Power Transformations to Improve Normality or Symmetry},
  author = {Yeo, In‐Kwon and Johnson, Richard A.},
  date = {2000-12-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {87},
  number = {4},
  pages = {954--959},
  issn = {0006-3444},
  doi = {10.1093/biomet/87.4.954},
  url = {https://doi.org/10.1093/biomet/87.4.954},
  urldate = {2022-02-28},
  abstract = {We introduce a new power transformation family which is well defined on the whole real line and which is appropriate for reducing skewness and to approximate normality. It has properties similar to those of the Box–Cox transformation for positive variables. The large‐sample properties of the transformation are investigated in the contect of a single random sample.},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/WNRJV2CV/232908.html}
}

@book{yong_stochastic_1999,
  title = {Stochastic {{Controls}}: {{Hamiltonian Systems}} and {{HJB Equations}}},
  shorttitle = {Stochastic {{Controls}}},
  author = {Yong, Jiongmin and Zhou, Xun Yu},
  date = {1999-06-22},
  publisher = {{Springer Science \& Business Media}},
  abstract = {As is well known, Pontryagin's maximum principle and Bellman's dynamic programming are the two principal and most commonly used approaches in solving stochastic optimal control problems. * An interesting phenomenon one can observe from the literature is that these two approaches have been developed separately and independently. Since both methods are used to investigate the same problems, a natural question one will ask is the fol lowing: (Q) What is the relationship betwccn the maximum principlc and dy namic programming in stochastic optimal controls? There did exist some researches (prior to the 1980s) on the relationship between these two. Nevertheless, the results usually werestated in heuristic terms and proved under rather restrictive assumptions, which were not satisfied in most cases. In the statement of a Pontryagin-type maximum principle there is an adjoint equation, which is an ordinary differential equation (ODE) in the (finite-dimensional) deterministic case and a stochastic differential equation (SDE) in the stochastic case. The system consisting of the adjoint equa tion, the original state equation, and the maximum condition is referred to as an (extended) Hamiltonian system. On the other hand, in Bellman's dynamic programming, there is a partial differential equation (PDE), of first order in the (finite-dimensional) deterministic case and of second or der in the stochastic case. This is known as a Hamilton-Jacobi-Bellman (HJB) equation.},
  isbn = {978-0-387-98723-1},
  langid = {english},
  pagetotal = {472},
  keywords = {Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes}
}

@misc{zhang_mlle_nodate,
  title = {{{MLLE}}: {{Modified Locally Linear Embedding Using Multiple Weights}}},
  shorttitle = {{{MLLE}}},
  author = {Zhang, Zhenyue and Wang, Jing},
  abstract = {The locally linear embedding (LLE) is improved by introducing multiple linearly independent local weight vectors for each neighborhood. We characterize the reconstruction weights and show the existence of the linearly independent weight vectors at each neighborhood. The modified locally linear embedding (MLLE) proposed in this paper is much stable. It can retrieve the ideal embedding if MLLE is applied on data points sampled from an isometric manifold. MLLE is also compared with the local tangent space alignment (LTSA). Numerical examples are given that show the improvement and efficiency of MLLE. 1},
  file = {/Users/joseluisaguilarcharfen/Zotero/storage/D48CEHUE/Zhang and Wang - MLLE Modified Locally Linear Embedding Using Mult.pdf;/Users/joseluisaguilarcharfen/Zotero/storage/QGVTR2VM/summary.html}
}

@report{zhang_sustainable_2019,
  type = {preprint},
  title = {Sustainable {{Nitrogen Management Index}}},
  author = {Zhang, Xin and Davidson, Eric},
  date = {2019-11-24},
  institution = {{Soil Science}},
  doi = {10.1002/essoar.10501111.1},
  url = {http://www.essoar.org/doi/10.1002/essoar.10501111.1},
  urldate = {2022-02-26},
  langid = {english}
}


